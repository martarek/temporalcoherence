\section{Results}
%DONE – Describe your experimental protocol (proportions used to divide the data into training, validation and test sets, data preprocessing, etc.).
The COIL100 dataset contains 100 different object. From this dataset, 30 objects were selected randomly to be the labeled examples. The other 70 objects were used for the video dataset. The remaining 30 objects were separated into three datasets. For each object, 4 angles were used for the training (0, 90, 180, 270 degrees) and 4 angles were used for the validation (45, 135, 225, 315 degrees). The rest of the angles were used as test data.

Since the article mentioned the pictures were 72X72 pixels, the images from the dataset were resized to this format prior to any experiment. The images were converted into grayscale format. The intra-image mean was substracted for each image. Each pixel was then divided by the intra-image standard deviation.
%TODO – Describe your procedure for selecting hyper-parameters.
%DONE – Present and analyze the results for different hyper-parameters on the validation and training sets.

\subsection{Results for different hyper-parameters}

The results for different hyper-parameters are presented in the table~\ref{tab:results}. They were all executed with a look-ahead of 10 epochs. It is interesting to note that the validation dataset is harder than the test dataset for most of the configurations. This is probably due to the fact that the validation angles were chosen to be the fartest from the training angles.

\begin{table}[f]
\label{tab:results}
\small
\begin{tabular}{@{}rrrrrrrrrrr@{}}
\toprule
\multicolumn{1}{l}{lr1} & \multicolumn{1}{l}{lr2} & \multicolumn{1}{l}{dc} & \multicolumn{1}{l}{sizes} & \multicolumn{1}{l}{best\_it} & \multicolumn{1}{l}{train1} & \multicolumn{1}{l}{valid1} & \multicolumn{1}{l}{test1} & \multicolumn{1}{l}{train2} & \multicolumn{1}{l}{valid2} & \multicolumn{1}{l}{test2} \\ \midrule
0.01 & 0.05 & 0.0E+00 & {[}12,48,176,672{]} & 73 & 0.058 & 0.358 & 0.233 & 0.278 & 1.254 & 0.859 \\
0.01 & 0.07 & 0.0E+00 & {[}12,48,176,672{]} & 15 & 0.858 & 0.875 & 0.878 & 3.343 & 3.349 & 3.348 \\
0.01 & 0.08 & 0.0E+00 & {[}12,48,176,672{]} & 14 & 0.875 & 0.875 & 0.880 & 3.351 & 3.357 & 3.355 \\
0.01 & 0.1 & 0.0E+00 & {[}12,48,176,672{]} & 72 & 0.058 & 0.367 & 0.239 & 0.307 & 1.267 & 0.885 \\
0.015 & 0.05 & 0.0E+00 & {[}12,48,176,672{]} & 53 & 0.083 & 0.358 & 0.241 & 0.281 & 1.296 & 0.883 \\
0.015 & 0.07 & 0.0E+00 & {[}12,48,176,672{]} & 8 & 0.858 & 0.875 & 0.871 & 3.367 & 3.371 & 3.370 \\
0.015 & 0.08 & 0.0E+00 & {[}12,48,176,672{]} & 52 & 0.075 & 0.350 & 0.240 & 0.341 & 1.261 & 0.900 \\
0.015 & 0.1 & 0.0E+00 & {[}12,48,176,672{]} & 7 & 0.858 & 0.883 & 0.870 & 3.373 & 3.377 & 3.376 \\
0.02 & 0.05 & 0.0E+00 & {[}12,48,176,672{]} & 56 & 0.017 & 0.317 & 0.171 & 0.065 & 1.185 & 0.681 \\
0.02 & 0.1 & 0.0E+00 & {[}12,48,176,672{]} & 56 & 0.017 & 0.333 & 0.184 & 0.059 & 1.231 & 0.730 \\
0.025 & 0.01 & 0.0E+00 & {[}10,40,160,640{]} & 52 & 0.025 & 0.325 & 0.184 & 0.103 & 1.356 & 0.787 \\
0.025 & 0.01 & 1.0E-07 & {[}10,40,160,640{]} & 53 & 0.017 & 0.325 & 0.184 & 0.082 & 1.301 & 0.760 \\
0.025 & 0.05 & 0.0E+00 & {[}10,40,160,640{]} & 57 & 0.008 & 0.317 & 0.179 & 0.039 & 1.280 & 0.716 \\
0.025 & 0.05 & 1.0E-07 & {[}10,40,160,640{]} & 42 & 0.050 & 0.333 & 0.201 & 0.250 & 1.412 & 0.900 \\
0.025 & 0.08 & 0.0E+00 & {[}10,40,160,640{]} & 44 & 0.042 & 0.317 & 0.190 & 0.202 & 1.345 & 0.850 \\
0.025 & 0.08 & 1.0E-07 & {[}10,40,160,640{]} & 51 & 0.025 & 0.333 & 0.177 & 0.112 & 1.250 & 0.744 \\
0.025 & 0.2 & 0.0E+00 & {[}10,40,160,640{]} & 54 & 0.017 & 0.325 & 0.189 & 0.065 & 1.242 & 0.734 \\
0.025 & 0.2 & 1.0E-07 & {[}10,40,160,640{]} & 43 & 0.042 & 0.342 & 0.207 & 0.203 & 1.401 & 0.891 \\
0.03 & 0.05 & 0.0E+00 & {[}12,48,176,672{]} & 44 & 0.025 & 0.300 & 0.173 & 0.116 & 1.229 & 0.742 \\
0.03 & 0.07 & 0.0E+00 & {[}12,48,176,672{]} & 50 & 0.025 & 0.317 & 0.172 & 0.053 & 1.160 & 0.699 \\
\textbf{0.03} & \textbf{0.08} & \textbf{0.0E+00} & \textbf{{[}12,48,176,672{]}} & \textbf{43} & \textbf{0.025} & \textbf{0.283} & \textbf{0.167} & \textbf{0.115} & \textbf{1.166} & \textbf{0.719} \\
0.03 & 0.1 & 0.0E+00 & {[}12,48,176,672{]} & 47 & 0.033 & 0.283 & 0.171 & 0.109 & 1.127 & 0.714 \\
0.05 & 0.01 & 0.0E+00 & {[}10,40,160,640{]} & 37 & 0.017 & 0.325 & 0.201 & 0.048 & 1.417 & 0.864 \\
0.05 & 0.01 & 1.0E-07 & {[}10,40,160,640{]} & 2 & 0.692 & 0.717 & 0.727 & 3.389 & 3.391 & 3.390 \\
0.05 & 0.05 & 0.0E+00 & {[}10,40,160,640{]} & 32 & 0.025 & 0.325 & 0.209 & 0.213 & 1.669 & 1.063 \\
0.05 & 0.05 & 1.0E-07 & {[}10,40,160,640{]} & 41 & 0.000 & 0.308 & 0.164 & 0.011 & 1.234 & 0.713 \\
0.05 & 0.08 & 0.0E+00 & {[}10,40,160,640{]} & 30 & 0.025 & 0.333 & 0.207 & 0.262 & 1.679 & 1.092 \\
0.05 & 0.08 & 1.0E-07 & {[}10,40,160,640{]} & 2 & 0.700 & 0.717 & 0.729 & 3.389 & 3.391 & 3.390 \\
0.05 & 0.2 & 0.0E+00 & {[}10,40,160,640{]} & 35 & 0.025 & 0.317 & 0.190 & 0.108 & 1.295 & 0.818 \\
0.05 & 0.2 & 1.0E-07 & {[}10,40,160,640{]} & 40 & 0.025 & 0.317 & 0.193 & 0.130 & 1.361 & 0.896 \\
0.09 & 0.01 & 0.0E+00 & {[}10,40,160,640{]} & 2 & 0.650 & 0.717 & 0.729 & 3.387 & 3.390 & 3.389 \\
0.09 & 0.01 & 1.0E-07 & {[}10,40,160,640{]} & 2 & 0.633 & 0.717 & 0.724 & 3.387 & 3.390 & 3.389 \\
0.09 & 0.05 & 0.0E+00 & {[}10,40,160,640{]} & 2 & 0.617 & 0.708 & 0.699 & 3.387 & 3.390 & 3.389 \\
0.09 & 0.05 & 1.0E-07 & {[}10,40,160,640{]} & 11 & 0.633 & 0.633 & 0.638 & 2.454 & 2.538 & 2.503 \\
0.09 & 0.08 & 0.0E+00 & {[}10,40,160,640{]} & 2 & 0.642 & 0.717 & 0.715 & 3.387 & 3.390 & 3.389 \\
0.09 & 0.08 & 1.0E-07 & {[}10,40,160,640{]} & 2 & 0.642 & 0.733 & 0.722 & 3.386 & 3.390 & 3.389 \\
0.09 & 0.2 & 0.0E+00 & {[}10,40,160,640{]} & 2 & 0.642 & 0.742 & 0.724 & 3.387 & 3.390 & 3.390 \\
0.09 & 0.2 & 1.0E-07 & {[}10,40,160,640{]} & 2 & 0.617 & 0.733 & 0.716 & 3.387 & 3.390 & 3.389 \\ \bottomrule
\end{tabular}
\caption{Results for different hyper-parameters. The best value is in bold.}
\end{table}

\subsection{Results details}
%DONE – Present any results that validate the correctness of your implementation.
The figure~\ref{fig:results_bestrun} presents the training and validation error for the best hyper-parameters. It is interesting to note the small decrease of the classification error at the begining of the optimization. This pattern was repeated with almost every selection of hyper-parameters. We countered this effect by increasing the value of the look-ahead.


\begin{figure}[htbf]
\label{fig:results_bestrun}
\includegraphics[width=0.48\textwidth]{result1.png} 
\includegraphics[width=0.48\textwidth]{result2.png} 
\caption{Training and validation error of the training procedure with the best validation error.}
\end{figure}

\subsection{Comparison}
%TODO – Present the results of your algorithm on the test set and make a comparison with an alternative, simple baseline.

To show the performance of the algorithm, we compared its performance against a convolutionnal-neural-network without the temporal coherence regularisation. The results for differents set of hyper-parameters are shown in table~\ref{tab:cnn}. We can see that the best value in validation in our method and the traditionnal method are roughly the same. However, the size of the neural network needed to achieve these performance are much smaller with temporal coherence. We can also see that there is more set of hyper-parameters that achieve interesting performance with this method than with the traditionnal method.

\begin{table}[f]
\begin{tabular}{@{}rrrrrrrrr@{}}
\toprule
\multicolumn{1}{l}{lr} & \multicolumn{1}{l}{sizes} & \multicolumn{1}{l}{best\_it} & \multicolumn{1}{l}{train1} & \multicolumn{1}{l}{valid1} & \multicolumn{1}{l}{test1} & \multicolumn{1}{l}{train2} & \multicolumn{1}{l}{valid2} & \multicolumn{1}{l}{test2} \\ \midrule
0.01 & {[}15,60,240,1700{]} & 9 & 0.833 & 0.867 & 0.851 & 3.383 & 3.386 & 3.385 \\
0.01 & {[}2,8,32,300{]} & 1 & 0.942 & 0.967 & 0.962 & 3.401 & 3.401 & 3.401 \\
0.01 & {[}200,100,50,25{]} & 1 & 0.892 & 0.908 & 0.908 & 3.401 & 3.401 & 3.401 \\
0.01 & {[}4,16,64,600{]} & 1 & 0.933 & 0.933 & 0.933 & 3.401 & 3.401 & 3.401 \\
0.01 & {[}8,32,128,1000{]} & 28 & 0.817 & 0.833 & 0.853 & 3.346 & 3.353 & 3.351 \\
0.03 & {[}100,200,400,1000{]} & 33 & 0.250 & 0.417 & 0.383 & 0.783 & 1.743 & 1.355 \\
0.03 & {[}15,60,240,1700{]} & 27 & 0.408 & 0.483 & 0.485 & 1.466 & 1.877 & 1.711 \\
0.03 & {[}2,8,32,300{]} & 5 & 0.925 & 0.958 & 0.948 & 3.395 & 3.396 & 3.396 \\
0.03 & {[}200,100,50,25{]} & 10 & 0.808 & 0.875 & 0.851 & 3.388 & 3.389 & 3.389 \\
0.03 & {[}4,16,64,600{]} & 4 & 0.933 & 0.933 & 0.933 & 3.394 & 3.395 & 3.395 \\
0.03 & {[}50,100,200,500{]} & 37 & 0.192 & 0.433 & 0.353 & 0.772 & 1.562 & 1.238 \\
0.03 & {[}8,32,128,1000{]} & 10 & 0.750 & 0.783 & 0.784 & 3.352 & 3.359 & 3.357 \\
\textbf{0.05} & \textbf{{[}100,200,400,1000{]}} & \textbf{42} & \textbf{0.017} & \textbf{0.275} & \textbf{0.163} & \textbf{0.070} & \textbf{1.249} & \textbf{0.755} \\
0.05 & {[}100,200,400,1000{]} & 35 & 0.000 & 0.325 & 0.186 & 0.002 & 1.683 & 0.942 \\
0.05 & {[}15,60,240,1700{]} & 8 & 0.725 & 0.717 & 0.719 & 3.218 & 3.228 & 3.225 \\
0.05 & {[}2,8,32,300{]} & 24 & 0.642 & 0.617 & 0.643 & 2.527 & 2.600 & 2.574 \\
0.05 & {[}200,100,75,50{]} & 19 & 0.483 & 0.550 & 0.530 & 1.357 & 1.914 & 1.669 \\
0.05 & {[}200,100,75,50{]} & 14 & 0.717 & 0.692 & 0.718 & 3.011 & 3.015 & 3.015 \\
0.05 & {[}4,16,64,600{]} & 8 & 0.842 & 0.883 & 0.874 & 3.367 & 3.371 & 3.370 \\
0.05 & {[}5,20,80,400{]} & 28 & 0.275 & 0.492 & 0.421 & 0.913 & 1.834 & 1.463 \\
0.05 & {[}5,20,80,400{]} & 7 & 0.850 & 0.892 & 0.873 & 3.376 & 3.379 & 3.379 \\
0.05 & {[}8,32,128,1000{]} & 12 & 0.708 & 0.725 & 0.722 & 3.073 & 3.086 & 3.081 \\
0.07 & {[}100,200,400,1000{]} & 11 & 0.675 & 0.675 & 0.690 & 2.690 & 2.791 & 2.757 \\
0.07 & {[}15,60,240,1700{]} & 6 & 0.658 & 0.692 & 0.686 & 3.246 & 3.255 & 3.252 \\
0.07 & {[}2,8,32,300{]} & 16 & 0.717 & 0.725 & 0.738 & 2.766 & 2.810 & 2.793 \\
0.07 & {[}200,100,50,25{]} & 3 & 0.825 & 0.850 & 0.860 & 3.396 & 3.397 & 3.397 \\
0.07 & {[}4,16,64,600{]} & 25 & 0.492 & 0.583 & 0.593 & 1.754 & 2.170 & 1.996 \\
0.07 & {[}50,100,200,500{]} & 36 & 0.025 & 0.292 & 0.183 & 0.211 & 1.350 & 0.878 \\
0.07 & {[}8,32,128,1000{]} & 9 & 0.683 & 0.683 & 0.687 & 3.065 & 3.078 & 3.073 \\
0.09 & {[}15,60,240,1700{]} & 7 & 0.667 & 0.683 & 0.691 & 2.871 & 2.879 & 2.876 \\
0.09 & {[}2,8,32,300{]} & 12 & 0.767 & 0.800 & 0.794 & 3.093 & 3.105 & 3.103 \\
0.09 & {[}200,100,50,25{]} & 3 & 0.800 & 0.858 & 0.844 & 3.395 & 3.396 & 3.396 \\
0.09 & {[}4,16,64,600{]} & 4 & 0.750 & 0.800 & 0.780 & 3.380 & 3.384 & 3.383 \\
0.09 & {[}8,32,128,1000{]} & 3 & 0.642 & 0.750 & 0.713 & 3.378 & 3.383 & 3.381 \\ \bottomrule
\end{tabular}
\caption{Results for the normal convolutionnal-neural network}
\label{tab:cnn}
\end{table}



